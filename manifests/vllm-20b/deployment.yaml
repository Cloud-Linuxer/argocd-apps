apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-gpt-oss-20b
spec:
  replicas: 1
  selector: { matchLabels: { app: vllm } }
  template:
    metadata: { labels: { app: vllm } }
    spec:
      runtimeClassName: nvidia
      hostIPC: true
      containers:
        - name: vllm
          image: vllm/vllm-openai:gptoss
          imagePullPolicy: Always
          args:
            - --model
            - openai/gpt-oss-20b
            # - --max-model-len
            # - "12090"          # 확실할 때만 사용
            - --dtype
            - bfloat16           # MXFP4는 bf16만 지원
            - --tensor-parallel-size
            - "1"
            - --gpu-memory-utilization
            - "0.90"
            - --download-dir
            - /models
            - --host
            - 0.0.0.0
            - --port
            - "8000"
          ports:
            - containerPort: 8000
          env:
            - name: NCCL_P2P_LEVEL
              value: "SYS"
            - name: VLLM_LOGGING_LEVEL
              value: "INFO"
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: VLLM_ATTENTION_BACKEND
              value: FLASHINFER          # Blackwell은 FlashInfer 경로
            - name: VLLM_USE_FLASHINFER_MOE_MXFP4_BF16
              value: "1"                 # MXFP4 MoE + sinks 경로 활성화
            - name: PYTORCH_CUDA_ALLOC_CONF
              value: "expandable_segments:True"
          volumeMounts:
            - name: hf-cache
              mountPath: /root/.cache/huggingface
            - name: models
              mountPath: /models
          resources:
            limits:
              nvidia.com/gpu: 1
      volumes:
        - name: hf-cache
          hostPath: { path: /home/gpt-oss/.cache/huggingface, type: DirectoryOrCreate }
        - name: models
          hostPath: { path: /home/gpt-oss/models, type: DirectoryOrCreate }
