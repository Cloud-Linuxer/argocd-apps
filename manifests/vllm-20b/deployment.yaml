apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-gpt-oss-20b
  labels:
    app: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm              # ← 기존 값 유지 (immutable)
  template:
    metadata:
      labels:
        app: vllm            # ← Pod 라벨도 동일
    spec:
      hostIPC: true
      runtimeClassName: nvidia
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.10.1
          imagePullPolicy: IfNotPresent
          args:
            - --model
            - openai/gpt-oss-20b
            - --dtype
            - bfloat16
            - --tensor-parallel-size
            - "1"
            - --gpu-memory-utilization
            - "0.90"
            - --host
            - 0.0.0.0
            - --port
            - "8000"
            - --download-dir
            - /models
          env:
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: VLLM_LOGGING_LEVEL
              value: INFO
            - name: NCCL_P2P_LEVEL
              value: SYS
            - name: PYTORCH_CUDA_ALLOC_CONF
              value: expandable_segments:True
            - name: VLLM_ATTENTION_BACKEND
              value: FLASHINFER
            # ★ Blackwell 전용(둘 중 하나만 선택). 우선 bf16 모드 권장
            - name: VLLM_USE_FLASHINFER_MXFP4_BF16_MOE
              value: "1"
          ports:
            - name: http
              containerPort: 8000
          volumeMounts:
            - { name: hf-cache, mountPath: /root/.cache/huggingface }
            - { name: models,   mountPath: /models }
            - { name: dshm,     mountPath: /dev/shm }
          resources:
            limits:
              nvidia.com/gpu: 1
          readinessProbe:
            httpGet: { path: /v1/models, port: http }
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 12
      volumes:
        - name: hf-cache
          hostPath: { path: /home/gpt-oss/.cache/huggingface, type: DirectoryOrCreate }
        - name: models
          hostPath: { path: /home/gpt-oss/models, type: DirectoryOrCreate }
        - name: dshm
          emptyDir: { medium: Memory, sizeLimit: 8Gi }
